{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3bc81d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.utils import pad_sequences\n",
    "docs = ['additional income',\n",
    "'best price',\n",
    "'big bucks',\n",
    "'cash bonus',\n",
    "'earn extra cash',\n",
    "'spring savings certificate',\n",
    "'valero gas marketing',\n",
    "'all domestic employees',\n",
    "'nominations for oct',\n",
    "'confirmation from spinner']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f34bcae",
   "metadata": {},
   "source": [
    "### one_hot() 함수는 각 단어를 고유한 정수로 인코딩하는 과정에서, 단어의 해시 값을 사용하여 정수로 변환한다. 해시 함수는 주어진 단어를 고정된 크기의 정수 범위로 매핑하는데, 이 과정에서 서로 다른 단어가 동일한 해시 값으로 매핑해서 출돌이 일어나 동일한 해시값으로 매필된 단어들이 같은 정수로 인코딩될수 있는 문제가 발생할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "15eb8f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11, 10], [26, 44], [14, 44], [15, 1], [31, 8, 15], [17, 39, 47], [26, 27, 22], [20, 15, 36], [24, 7, 15], [9, 39, 40]]\n"
     ]
    }
   ],
   "source": [
    "labels = np.array([1,1,1,1,1,0,0,0,0,0])\n",
    "vocab_size = 50\n",
    "encoded_docs = [one_hot(d, vocab_size) for d in docs]\n",
    "print(encoded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741d8c71",
   "metadata": {},
   "source": [
    "### 해시 충돌로 인해 발생하는 이런 문제를 해결하기 위해서는 해싱 대신에 다른 단어를 인코딩 하는 방법을 사용해야 한다.\n",
    "### 대표적으로는 단어를 정수로 매핑하는 단어 집합을 만들고 각 단어에 고유한 정수를 할당하는 방법이 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f0e8e091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합 :  {'cash': 1, 'additional': 2, 'income': 3, 'best': 4, 'price': 5, 'big': 6, 'bucks': 7, 'bonus': 8, 'earn': 9, 'extra': 10, 'spring': 11, 'savings': 12, 'certificate': 13, 'valero': 14, 'gas': 15, 'marketing': 16, 'all': 17, 'domestic': 18, 'employees': 19, 'nominations': 20, 'for': 21, 'oct': 22, 'confirmation': 23, 'from': 24, 'spinner': 25} \n",
      "\n",
      "texts_to_sequences :  [[2, 3], [4, 5], [6, 7], [1, 8], [9, 10, 1], [11, 12, 13], [14, 15, 16], [17, 18, 19], [20, 21, 22], [23, 24, 25]] \n",
      "\n",
      "shape =  (10, 3) \n",
      "\n",
      "[[ 2  3  0]\n",
      " [ 4  5  0]\n",
      " [ 6  7  0]\n",
      " [ 1  8  0]\n",
      " [ 9 10  1]\n",
      " [11 12 13]\n",
      " [14 15 16]\n",
      " [17 18 19]\n",
      " [20 21 22]\n",
      " [23 24 25]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "t= Tokenizer()\n",
    "t.fit_on_texts(docs)\n",
    "print(\"단어 집합 : \",t.word_index,\"\\n\")\n",
    "\n",
    "encoded_docs = t.texts_to_sequences(docs)\n",
    "print(\"texts_to_sequences : \",encoded_docs,\"\\n\")\n",
    "\n",
    "vocab_size = len(t.word_index) + 1 # 1을 더해주는 이유는 1부터 시작하기 위해서\n",
    "\n",
    "max_length =3\n",
    "\n",
    "padded_docs= pad_sequences(encoded_docs,maxlen=max_length,padding='post')\n",
    "print('shape = ',padded_docs.shape,\"\\n\")\n",
    "print(padded_docs,\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f796fe2",
   "metadata": {},
   "source": [
    "* fit_on_texts()는 Tokenizer 클래스의 메서드로, 주어진 텍스트 데이터를 기반으로 단어 집합을 생성하는 역할을 한다. 이 메서드는 입력으로 리스트 형태의 텍스트 데이터를 받는다. 이 데이터를 사용하여 Tokenizer 객체 내부의 상태를 업데이트하고, 단어 집합을 구축한다. 각 텍스트를 단어로 분리되고, 이 단어들을 기반으로 단어 집합이 형성된다.\n",
    "\n",
    "* word_index는 Tokenizer 객체의 속성으로, 단어와 해당 단어의 정수 인덱스를 매핑한 딕셔너리이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c6561b89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1/1 [==============================] - 1s 964ms/step - loss: 0.6963 - accuracy: 0.5000\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.6934 - accuracy: 0.6000\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.6906 - accuracy: 0.6000\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.6878 - accuracy: 0.7000\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.6850 - accuracy: 0.7000\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.6822 - accuracy: 0.7000\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6795 - accuracy: 0.8000\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6767 - accuracy: 0.8000\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6740 - accuracy: 0.9000\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6712 - accuracy: 0.9000\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.6684 - accuracy: 0.9000\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6657 - accuracy: 0.9000\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6629 - accuracy: 0.9000\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6602 - accuracy: 0.9000\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6574 - accuracy: 0.9000\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6546 - accuracy: 0.9000\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6518 - accuracy: 0.9000\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6490 - accuracy: 1.0000\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6462 - accuracy: 1.0000\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6434 - accuracy: 1.0000\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6406 - accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6378 - accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6349 - accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.6321 - accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6292 - accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6263 - accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6234 - accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6205 - accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6176 - accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6146 - accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6116 - accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6087 - accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6057 - accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6026 - accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5996 - accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5965 - accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5935 - accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.5904 - accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.5873 - accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5841 - accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5810 - accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.5778 - accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5746 - accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.5714 - accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5682 - accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.5650 - accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5617 - accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5584 - accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.5551 - accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5518 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 173ms/step - loss: 0.5485 - accuracy: 1.0000\n",
      "정확도 =  1.0\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding (vocab_size,8,input_length = max_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(padded_docs,labels,epochs=50,verbose=1)\n",
    "\n",
    "loss,accuracy=model.evaluate(padded_docs, labels,verbose=1)\n",
    "\n",
    "print('정확도 = ',accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4eb03bf",
   "metadata": {},
   "source": [
    "* model = Sequential() 모델 객체를 생성한다. Sequential 모델은 각 레이어를 순차적으로 쌓아 구성하는 방식이다.\n",
    "* model.add(Embedding (vocab_size,8,input_length = max_length)) : Embedding 레이어를 모델에 추가한다. Embedding은 단어를 밀집 벡터로 변환하는 역할을 한다.\n",
    "* model.add(Flatten()) : Flatten 레이어를 모델에 추가한다. Flatten은 다차원 입력을 1차원으로 변환한다. Embedding레이어의 출력을 Flatten 레이어로 전달하기 위해 사용된다.\n",
    "* model.add(Dense(1,activation='sigmoid')) : Dense 레이어를 모델에 추가한다. Dense 레이어는 fully connected 레이어를 의미하며, 1개의 뉴런과 sigmoid 활성화 함수를 가지고 있고, 이 레이어는 이진 분류를 위해 사용된다.\n",
    "* model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) : 모델을 컴파일한다.'adam' 옵티마이저를 사용하고, 손실 함수로는 이진 분류에서 자주 사용되는 'binary_crossentropy'를 선택한다. 정확도를 측정하기 위해 'accuracy' 메트릭을 사용한다\n",
    "* model.fit(padded_docs,labels,epochs=50,verbose=0) : 모델을 학습시킨다. padded_docs는 패딩이 적용된 입력 데이터, labels는 해당하는 레이블 데이터이다. epochs = 50 는 전체 데이터셋을 50번 반복하여 학습하는 것을 의미한다. verbose = 0 은 학습과정의 로그를 출력하지 않음을 의미한다.\n",
    "* loss,accuracy=model.evaluate(padded_docs, labels,verbose=1) : 학습된 모델을 평가한다. padded_docs와 labels를 이용하여 모델의 손실과 정확도를 평가한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "34823407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합 :  {'income': 1, 'big': 2, 'how': 3} \n",
      "\n",
      "texts_to_sequences :  [[2, 1], [1, 3]] \n",
      "\n",
      "shape =  (2, 3) \n",
      "\n",
      "[[2 1 0]\n",
      " [1 3 0]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_doc= ['big income','income how']\n",
    "\n",
    "t= Tokenizer()\n",
    "t.fit_on_texts(test_doc)\n",
    "print(\"단어 집합 : \",t.word_index,\"\\n\")\n",
    "\n",
    "encoded_docs = t.texts_to_sequences(test_doc)\n",
    "print(\"texts_to_sequences : \",encoded_docs,\"\\n\")\n",
    "\n",
    "vocab_size = len(t.word_index) + 1 # 1을 더해주는 이유는 1부터 시작하기 위해서\n",
    "\n",
    "max_length =3\n",
    "\n",
    "padded_docs= pad_sequences(encoded_docs,maxlen=max_length,padding='post')\n",
    "print('shape = ',padded_docs.shape,\"\\n\")\n",
    "print(padded_docs,\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5087395f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 17ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.54078496],\n",
       "       [0.5589574 ]], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(padded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1602538d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 1, 0],\n",
       "       [1, 3, 0]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0685f01a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
